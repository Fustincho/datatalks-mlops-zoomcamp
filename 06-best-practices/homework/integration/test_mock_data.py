import os
import subprocess

import boto3
import pandas as pd

bucket = "taxi-nyc-duration"

# Run the batch processing script with the test data
subprocess.run(
    [
        "pipenv",
        "run",
        "python",
        "batch_processing.py",
        "--year",
        "2023",
        "--month",
        "3",
        "--metrics",
        "--bucket",
        bucket,
        "--test",
    ],
    check=True,
)

S3_ENDPOINT_URL = os.getenv("S3_ENDPOINT_URL")

options = {"client_kwargs": {"endpoint_url": S3_ENDPOINT_URL}}

# Load the output file generated by the script
output_file = "9999/99/taxi_type=yellow_year=9999_month=99.parquet"

s3 = boto3.client("s3", endpoint_url=S3_ENDPOINT_URL)
response = s3.head_object(Bucket=bucket, Key=output_file)
size = response["ContentLength"]
print(f"Homework Q5: File size in S3: {size} bytes")

df_result = pd.read_parquet(f"s3://{bucket}/{output_file}", storage_options=options)

# Verify the structure of the output
assert "ride_id" in df_result.columns, "'ride_id' column is missing from the output."
assert "y_pred" in df_result.columns, "'y_pred' column is missing from the output."

print(f'Homework Q6: Sum of predictions: {df_result["y_pred"].sum()}')
