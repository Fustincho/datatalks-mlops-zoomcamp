{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27205881",
   "metadata": {},
   "source": [
    "# Homework 4: batch processing of the Taxi Dataset\n",
    "\n",
    "The objective of this assignment is to utilize a pre-trained model to develop a batch processing pipeline, which will be deployed via a Docker container. The user will execute the Docker container with specific parameters to run the processing pipeline.\n",
    "\n",
    "Additionally, the pipeline will be modified to export its output to an S3 bucket. The entire pipeline will then be recreated using Mage and its execution will be triggered.\n",
    "\n",
    "**Additional Comments:**\n",
    "\n",
    "- The model was originally trained using `scikit-learn==1.5.0`, and it is essential to maintain the same version to ensure compatibility. The `model.bin` pickle file contains both the ML model and the dictionary vectorizer.\n",
    "\n",
    "- The environment for this homework was created using pipenv. The `Pipfile` and `Pipfile.lock` were also attached to the homework folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c51efaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn==1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d5031",
   "metadata": {},
   "source": [
    "The following code chunks mirror the processing from previous homeworks. We load the model, download the data, filter out outliers based on trip duration, and then make predictions using this model. Note that it is possible to parametrize the data download process by setting the `year` and `month`, and passing these parameters directly to the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef880a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open('model.bin', 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c08294",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    \n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4854399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2023\n",
    "month = 3\n",
    "df = read_data(f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669fda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = df[categorical].to_dict(orient='records')\n",
    "X_val = dv.transform(dicts)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873e56c",
   "metadata": {},
   "source": [
    "**Question 1:** What's the standard deviation of the predicted duration for this (March 2023) dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf532ae7-1897-428c-ba0c-875ccaf7d76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.247488852238703"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1df7d",
   "metadata": {},
   "source": [
    "Now we creat an artificial `ride_id` column and, along with the predictions, create a `DataFrame` with the results and save it as a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfce114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e66f904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame({'ride_id': df['ride_id'], 'y_pred': y_pred})\n",
    "\n",
    "output_file = 'output.parquet'\n",
    "\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044ce40",
   "metadata": {},
   "source": [
    "**Question 2:** What's the size of the output file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762074a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.46183013916016"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_size_bytes = os.path.getsize(output_file)\n",
    "file_size_mb = file_size_bytes / (1024 ** 2)\n",
    "file_size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f065179",
   "metadata": {},
   "source": [
    "That's pretty much the whole pipeline. Now we're instructed to turn the nb into a script.\n",
    "\n",
    "**Question 3:** Which command you need to execute for that?\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to script notebook_name.ipynb\n",
    "```\n",
    "\n",
    "The resulting script was further transformed based on the remaining part of the homework. You can find the complete script at `batch_processing.py`\n",
    "\n",
    "As mentioned before, pipenv was used to install all the libraries by running\n",
    "\n",
    "```bash\n",
    "pip install pipenv\n",
    "pipenv --python 3.10.13 # initialize the environment\n",
    "pipenv install pyarrow==16.1.0 scikit-learn==1.5.0 pandas==2.2.2\n",
    "```\n",
    "\n",
    "**Question 4:** After installing the libraries, pipenv creates two files: `Pipfile` and `Pipfile.lock`. The `Pipfile.lock` file keeps the hashes of the dependencies we use for the virtual env. What's the first hash for the Scikit-Learn dependency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17fe75e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first hash of scikit-learn is: sha256:057b991ac64b3e75c9c04b5f9395eaf19a6179244c089afdebaad98264bff37c\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('Pipfile.lock') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "scikit_learn_hashes = data['default']['scikit-learn']['hashes']\n",
    "first_scikit_learn_hash = scikit_learn_hashes[0]\n",
    "\n",
    "print(f\"The first hash of scikit-learn is: {first_scikit_learn_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ecfef7",
   "metadata": {},
   "source": [
    "The script was parametrized using `Click`. It accepts the following parameters:\n",
    "\n",
    "+ `--year`: Enter a year\n",
    "+ `--month`: Enter a month (1-12)\n",
    "+ `--metrics`: Print metrics about the predictions\n",
    "+ `--bucket`: Name of the S3 bucket to upload the results\n",
    "\n",
    "**Question 5:** Run the script for April 2023. What's the mean predicted duration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a232a9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-14 11:20:00,974 - INFO - Starting data processing for 2023-04\n",
      "2024-06-14 11:20:00,974 - INFO - Loading and processing data...\n",
      "2024-06-14 11:20:12,420 - INFO - Data loaded successfully. Proceeding with predictions.\n",
      "2024-06-14 11:20:12,421 - INFO - Transforming categorical variables...\n",
      "2024-06-14 11:20:19,048 - INFO - Generating predictions using the model...\n",
      "2024-06-14 11:20:19,057 - INFO - Predictions generated successfully.\n",
      "2024-06-14 11:20:19,057 - INFO - Calculating prediction metrics...\n",
      "2024-06-14 11:20:19,065 - INFO - Prediction metrics - Mean: 14.292283, Std: 6.353997, Min: -16.328844, Max: 70.047721\n",
      "2024-06-14 11:20:19,611 - INFO - Preparing results for export...\n",
      "2024-06-14 11:20:19,631 - INFO - Exporting results to output.parquet...\n",
      "2024-06-14 11:20:19,896 - INFO - Data processing and export completed successfully.\n"
     ]
    }
   ],
   "source": [
    "! python3 batch_processing.py --year 2023 --month 4 --metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e5451",
   "metadata": {},
   "source": [
    "Finally, we'll package the script in the docker container. For that, we'll a base image that was prepared by the course tutor: [agrigorev/zoomcamp-model:mlops-2024-3.10.13-slim](https://hub.docker.com/layers/agrigorev/zoomcamp-model/mlops-2024-3.10.13-slim/images/sha256-f54535b73a8c3ef91967d5588de57d4e251b22addcbbfb6e71304a91c1c7027f?context=repo)\n",
    "\n",
    "This is what the content of this image is:\n",
    "\n",
    "```DockerFile\n",
    "FROM python:3.10.13-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY [ \"model2.bin\", \"model.bin\" ]\n",
    "```\n",
    "\n",
    "This image already has a pickle file with a dictionary vectorizer and a model, so we use `agrigorev`'s image as base and we don't include our `model.bin` into the container. We build this container by running\n",
    "\n",
    "```bash\n",
    "docker build -t homework-4 .\n",
    "```\n",
    "\n",
    "**Question 6:** Now run the script with docker. What's the mean predicted duration for May 2023?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fdee2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-14 16:20:26,007 - INFO - Starting data processing for 2023-05\n",
      "2024-06-14 16:20:26,008 - INFO - Loading and processing data...\n",
      "2024-06-14 16:20:38,461 - INFO - Data loaded successfully. Proceeding with predictions.\n",
      "2024-06-14 16:20:38,466 - INFO - Transforming categorical variables...\n",
      "2024-06-14 16:20:52,329 - INFO - Generating predictions using the model...\n",
      "2024-06-14 16:20:52,342 - INFO - Predictions generated successfully.\n",
      "2024-06-14 16:20:52,342 - INFO - Calculating prediction metrics...\n",
      "2024-06-14 16:20:52,351 - INFO - Prediction metrics - Mean: 0.191744, Std: 1.388140, Min: -5.206588, Max: 5.559183\n",
      "2024-06-14 16:20:53,256 - INFO - Preparing results for export...\n",
      "2024-06-14 16:20:53,272 - INFO - Exporting results to output.parquet...\n",
      "2024-06-14 16:20:53,674 - INFO - Data processing and export completed successfully.\n"
     ]
    }
   ],
   "source": [
    "! docker run --platform linux/amd64 -it homework-4 --year 2023 --month 5 --metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e468ca3",
   "metadata": {},
   "source": [
    "## Bonus: upload the result the cloud\n",
    "\n",
    "To achieve this part of the homework, we now create, in AWS:\n",
    "\n",
    "- An S3 bucket called `mlops-zoomcamp-fustincho`.\n",
    "- An IAM User and create access keys for it.\n",
    "- A policy that is attached directly to the user, with the following permissions:\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"Version\": \"2012-10-17\",\n",
    "\t\"Statement\": [\n",
    "\t\t{\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Action\": \"s3:ListBucket\",\n",
    "\t\t\t\"Resource\": \"arn:aws:s3:::mlops-zoomcamp-fustincho\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Action\": \"s3:PutObject\",\n",
    "\t\t\t\"Resource\": \"arn:aws:s3:::mlops-zoomcamp-fustincho/*\"\n",
    "\t\t}\n",
    "\t]\n",
    "}\n",
    "```\n",
    "\n",
    "When `batch_processing.py` receives a bucket name as the `--bucket` parameter, then it attempts to upload, via the boto3 client, the resulting output. For this to work, it is imperative for `boto3` to find the AWS credentials. For this, we create an `env` file and paste the access keys we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d66acafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "AWS_ACCESS_KEY_ID=your-aws-access-key-id\n",
    "AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key\n",
    "AWS_DEFAULT_REGION=your-region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "868f37bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-14 16:29:26,910 - INFO - Starting data processing for 2023-03\n",
      "2024-06-14 16:29:26,910 - INFO - Loading and processing data...\n",
      "2024-06-14 16:29:39,709 - INFO - Data loaded successfully. Proceeding with predictions.\n",
      "2024-06-14 16:29:39,710 - INFO - Transforming categorical variables...\n",
      "2024-06-14 16:29:51,171 - INFO - Generating predictions using the model...\n",
      "2024-06-14 16:29:51,183 - INFO - Predictions generated successfully.\n",
      "2024-06-14 16:29:51,183 - INFO - Calculating prediction metrics...\n",
      "2024-06-14 16:29:51,192 - INFO - Prediction metrics - Mean: 0.188769, Std: 1.391873, Min: -5.299526, Max: 5.559183\n",
      "2024-06-14 16:29:52,092 - INFO - Preparing results for export...\n",
      "2024-06-14 16:29:52,111 - INFO - Exporting results to output.parquet...\n",
      "2024-06-14 16:29:52,512 - INFO - Data processing and export completed successfully.\n",
      "2024-06-14 16:29:52,512 - INFO - Uploading results to S3 bucket: mlops-zoomcamp-fustincho\n",
      "2024-06-14 16:29:52,534 - INFO - Found credentials in environment variables.\n",
      "2024-06-14 16:30:13,588 - INFO - File uploaded successfully to S3 at 2023/03/output.parquet.\n"
     ]
    }
   ],
   "source": [
    "!docker run --platform linux/amd64 --env-file .env homework-4 --year 2023 --month 3 --metrics --bucket mlops-zoomcamp-fustincho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbeae0c",
   "metadata": {},
   "source": [
    "![](./img/s3_uploaded.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970f4e8",
   "metadata": {},
   "source": [
    "## Bonus: use Mage for batch inference\n",
    "\n",
    "The `../bonus-homework` folder contains the solution for this bonus homework, using the same model we used for the script. To run the mage project we use `docker compose up`. In Mage, we create a pipeline and separate parts of the `batch_processing.py` in blocks:\n",
    "\n",
    "![](./img/pipeline.png)\n",
    "\n",
    "The `year` and `month` were also added as global variables within the pipeline.\n",
    "\n",
    "Then we create a trigger that we will use to execute the pipeline:\n",
    "\n",
    "![](./img/trigger.png)\n",
    "\n",
    "With this API endpoint active, it is possible to trigger a batch processing job by running\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:6789/api/pipeline_schedules/1/pipeline_runs/5a78a51f345442b0b5ebee3cd941cd0f \\\n",
    "  --header 'Content-Type: application/json' \\\n",
    "  --data '\n",
    "{\n",
    "  \"pipeline_run\": {\n",
    "    \"variables\": {\n",
    "      \"year\": \"2023\",\n",
    "      \"month\": \"05\"\n",
    "    }\n",
    "  }\n",
    "}'\n",
    "```\n",
    "\n",
    "We will now test this using the `requests` module. We will predict the trip duration for May and June 2023:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74676ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'pipeline_run': {'id': 5, 'created_at': '2024-06-14 16:40:58', 'updated_at': '2024-06-14 16:40:58', 'pipeline_schedule_id': 1, 'pipeline_uuid': 'batch_process_taxi_dataset', 'execution_date': '2024-06-14 16:40:58.983406', 'status': 'initial', 'started_at': None, 'completed_at': None, 'variables': {'year': '2023', 'month': '05', 'execution_partition': '1/20240614T164058_983406'}, 'passed_sla': False, 'event_variables': {}, 'metrics': None, 'backfill_id': None, 'executor_type': 'local_python'}}\n",
      "200\n",
      "{'pipeline_run': {'id': 6, 'created_at': '2024-06-14 16:40:59', 'updated_at': '2024-06-14 16:40:59', 'pipeline_schedule_id': 1, 'pipeline_uuid': 'batch_process_taxi_dataset', 'execution_date': '2024-06-14 16:40:59.037369', 'status': 'initial', 'started_at': None, 'completed_at': None, 'variables': {'year': '2023', 'month': '06', 'execution_partition': '1/20240614T164059_037369'}, 'passed_sla': False, 'event_variables': {}, 'metrics': None, 'backfill_id': None, 'executor_type': 'local_python'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:6789/api/pipeline_schedules/1/pipeline_runs/5a78a51f345442b0b5ebee3cd941cd0f\"\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "data = {\n",
    "    \"pipeline_run\": {\n",
    "        \"variables\": {\n",
    "            \"year\": \"2023\",\n",
    "            \"month\": \"05\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())\n",
    "\n",
    "data[\"pipeline_run\"][\"variables\"][\"month\"] = \"06\"\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66617e",
   "metadata": {},
   "source": [
    "![](./img/trigger_running.png)\n",
    "\n",
    "Once it is completed, we can find the predictions here `../bonus-homework/mage_project/`:\n",
    "\n",
    "![](./img/trigger_complete.png)\n",
    "\n",
    "![](./img/outputs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2677b764",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
